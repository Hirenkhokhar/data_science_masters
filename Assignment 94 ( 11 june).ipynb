{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d08f1a6",
   "metadata": {},
   "source": [
    "## Part l: Uderstanding Regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0d85e4",
   "metadata": {},
   "source": [
    "<b>Q1 What is regularization in the context of deep learning ? Why is it important?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad9e431",
   "metadata": {},
   "source": [
    "the context of deep learning, regularization refers to the techniques and methods used to prevent overfitting, a common problem where a model learns to perform well on the training data but fails to generalize to new, unseen data. Overfitting occurs when a model becomes overly complex, capturing noise and idiosyncrasies in the training data rather than the underlying patterns that should generalize to new examples. Regularization techniques are essential to improve the generalization ability of deep learning models and prevent overfitting.\n",
    "\n",
    "Importance of Regularization:\n",
    "\n",
    "Generalization: The ultimate goal of machine learning, including deep learning, is to develop models that can accurately predict or classify new, unseen data. Regularization methods help ensure that the learned representations capture true underlying patterns, leading to better generalization to previously unseen examples.\n",
    "\n",
    "Complexity Control: Deep neural networks are capable of learning complex relationships in data. However, too much complexity can lead to overfitting. Regularization techniques help control the complexity of the learned models, preventing them from fitting noise in the training data.\n",
    "\n",
    "Avoiding Overfitting: Overfitting occurs when a model becomes too specific to the training data and loses its ability to generalize. Regularization techniques mitigate this issue by introducing constraints or penalties that discourage the model from fitting noise.\n",
    "\n",
    "Reducing Model Variance: Overfitting often results in high variance, meaning the model's performance can vary significantly when exposed to different datasets. Regularization helps reduce this variance by promoting simpler models that generalize better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd931e74",
   "metadata": {},
   "source": [
    "<b>Q2 Explain the bias-variance tradeoff and how regularization helps in addressing this tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b564e0a",
   "metadata": {},
   "source": [
    "The bias-variance tradeoff is a fundamental concept in machine learning that relates to the performance of a model on both the training data and unseen data (testing data). It highlights the balance between two sources of error: bias and variance.\n",
    "\n",
    "Bias:\n",
    "\n",
    "Bias refers to the error due to overly simplistic assumptions in the learning algorithm. A high bias model tends to underfit the data, meaning it doesn't capture the underlying patterns or complexities in the data. It is overly biased towards its assumptions and fails to learn the true relationships in the data.\n",
    "Variance:\n",
    "\n",
    "Variance refers to the error due to the model's sensitivity to small fluctuations in the training data. A high variance model fits the training data very closely, even capturing noise and outliers. However, this makes the model less likely to generalize well to new, unseen data because it has learned to accommodate the idiosyncrasies of the training dataset.\n",
    "Bias-Variance Tradeoff:\n",
    "\n",
    "As a model becomes more complex (e.g., deeper neural networks, higher-degree polynomial regression), its ability to fit the training data closely increases. However, this complexity can also make the model more sensitive to variations in the data.\n",
    "A high-bias model tends to have a simple structure that might not capture the underlying relationships in the data, leading to poor training and testing performance.\n",
    "A high-variance model, on the other hand, fits the training data very closely but might generalize poorly to new data due to its sensitivity to noise.\n",
    "The goal is to strike a balance between bias and variance to achieve a model that generalizes well to new data while still capturing the underlying patterns.\n",
    "Role of Regularization:\n",
    "Regularization techniques help address the bias-variance tradeoff by introducing constraints or penalties on the model's complexity during training. They prevent the model from becoming overly complex and focus it on learning relevant patterns while avoiding noise.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeb8d39",
   "metadata": {},
   "source": [
    "<b>Q3 Describe the concept of L1 and L2 regularization. How do they differ in terms of penalty calculation and\n",
    "their effects on the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704850d6",
   "metadata": {},
   "source": [
    "L1 and L2 regularization are techniques used in machine learning and statistics to prevent overfitting and improve the generalization of models, particularly in regression and classification tasks. They achieve this by adding a penalty term to the model's objective function, which encourages the model to have smaller parameter values.\n",
    "\n",
    "L1 Regularization (Lasso):\n",
    "In L1 regularization, also known as the Lasso (Least Absolute Shrinkage and Selection Operator), a penalty term is added to the objective function, which is proportional to the absolute values of the model's coefficients (parameters).\n",
    "\n",
    "                  L1 regularization term = λ * Σ|θi|\n",
    "λ is the regularization parameter that controls the strength of the penalty, and θi represents the model's coefficients.\n",
    "\n",
    "L1 regularization has the effect of driving some of the coefficients to exactly zero, effectively performing feature selection. This means that L1 regularization can be used to perform automatic feature selection, as it tends to set some of the less important features' coefficients to zero. As a result, L1 regularization can lead to sparse models where only a subset of the features is used for prediction.\n",
    "\n",
    "L2 Regularization (Ridge):\n",
    "In L2 regularization, also known as Ridge regularization, the penalty term is proportional to the square of the model's coefficients. Mathematically, the L2 regularization term can be represented as the sum of the squared values of the coefficients:\n",
    "\n",
    "                        L2 regularization term = λ * Σθi^2\n",
    "Similar to L1 regularization, λ is the regularization parameter that controls the strength of the penalty, and θi represents the model's coefficients.\n",
    "\n",
    "L2 regularization encourages the model's coefficients to be small but does not typically drive them all the way to zero. Instead, it leads to smaller and more evenly distributed coefficients, effectively reducing the impact of any individual feature. This can help prevent the model from becoming overly sensitive to noise in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61437371",
   "metadata": {},
   "source": [
    "<b>Q4 Discuss the role of regularization in preventing overfitting and improving the generalization of deep\n",
    "learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade18777",
   "metadata": {},
   "source": [
    "Regularization plays a crucial role in preventing overfitting and improving the generalization of deep learning models. Overfitting occurs when a model learns to perform exceptionally well on the training data but fails to generalize to new, unseen data. Regularization techniques help address this issue by introducing constraints on the model's complexity and parameter values, which encourages it to learn more robust and generalizable representations. In the context of deep learning models, such as neural networks, the effects of regularization are particularly important due to the high capacity of these models.\n",
    "\n",
    "Parameter Constraints:\n",
    "Regularization techniques add penalty terms to the loss function that the model tries to minimize. These penalty terms are based on the complexity or magnitude of the model's parameters. By limiting the size of parameter values, regularization prevents the model from fitting noise and outliers present in the training data. This helps the model focus on learning meaningful patterns and relationships in the data, leading to better generalization.\n",
    "\n",
    "Reduced Model Complexity:\n",
    "Deep learning models have a large number of parameters, which gives them the flexibility to capture intricate patterns in the training data. However, this very flexibility can make them prone to overfitting, as they might memorize noise in the data. Regularization techniques like L1 and L2 regularization, as well as techniques like dropout, reduce the effective complexity of the model by discouraging the growth of individual parameter magnitudes. This promotes simpler models that are less likely to overfit.\n",
    "\n",
    "Feature Selection:\n",
    "Some regularization techniques, particularly L1 regularization, encourage certain features to have exactly zero coefficients. This leads to feature selection, where the model automatically identifies and focuses on the most relevant features while ignoring irrelevant or noisy ones. This process can lead to more interpretable models and further prevents overfitting by reducing the model's reliance on potentially noisy features.\n",
    "\n",
    "Smoothing Effect:\n",
    "Regularization methods, especially L2 regularization, distribute the parameter values more evenly across different features. This smoothing effect makes the model less sensitive to minor fluctuations in the training data, making it more robust to noise. Consequently, the model is less likely to overfit to the specific examples in the training set and can better generalize to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0401b30",
   "metadata": {},
   "source": [
    "## Part 2: Regularization Technique "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738a3185",
   "metadata": {},
   "source": [
    "<b>Q5  Explain Dropout regularization and how it works to reduce overfitting. Discuss the impact of Dropout on\n",
    "model training and inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0edbd5c",
   "metadata": {},
   "source": [
    "Dropout is a regularization technique used in neural networks to reduce overfitting by preventing complex co-adaptations of neurons within the same layer. It involves randomly \"dropping out\" (deactivating) a subset of neurons during each training iteration. This conceptually simulates training multiple subnetworks with different configurations and encourages the network to be more robust and less reliant on any individual neuron's activation. Dropout was introduced by Srivastava et al. in their 2014 paper \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting.\"\n",
    "\n",
    "Here's how Dropout works and its impact on model training and inference:\n",
    "\n",
    "Dropout during Training:\n",
    "\n",
    "Inactivation of Neurons: During each training iteration, a certain fraction of neurons in a layer are randomly selected to be dropped out. This means their outputs are set to zero, effectively deactivating them for that iteration.\n",
    "\n",
    "Stochastic Process: The dropout process is stochastic, meaning the selection of which neurons to drop out is random and can change from iteration to iteration. The fraction of neurons dropped out is determined by a parameter called the dropout rate, typically between 0.2 and 0.5.\n",
    "\n",
    "Network Ensemble: By randomly dropping out different neurons in each iteration, the network is effectively trained on a variety of smaller subnetworks. This ensemble-like behavior helps prevent the network from becoming overly specialized to the training data and encourages it to learn more robust and generalizable features.\n",
    "\n",
    "Impact of Dropout on Training:\n",
    "\n",
    "Reduced Co-adaptation: Dropout discourages complex co-adaptations between neurons. Since any neuron might be dropped out, neurons are forced to learn useful features independently. This helps prevent the network from fitting noise and capturing specific relationships in the training data that might not generalize well.\n",
    "\n",
    "Regularization: Dropout acts as a form of regularization by adding noise to the training process. This reduces the risk of overfitting by preventing the network from memorizing the training data.\n",
    "\n",
    "Smaller Effective Network: Each iteration of training has a smaller network due to the dropped-out neurons. This prevents the model from becoming excessively large and overfitting the data.\n",
    "\n",
    "Impact of Dropout on Inference:\n",
    "During inference (testing or prediction), dropout is not applied. However, to maintain the expected behavior of the model learned during training, the output of each neuron is scaled down by the dropout rate. This accounts for the fact that during training, neurons were active only with a certain probability. This scaled-down output ensures that the total signal received by each neuron remains approximately consistent between training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b25c405",
   "metadata": {},
   "source": [
    "<b>Q6 Describe the concept of Early stopping as a form of regularization. How does it help prevent overfitting\n",
    "during the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82161d5d",
   "metadata": {},
   "source": [
    "\n",
    "Early stopping is a regularization technique commonly used in machine learning, particularly in the training of deep learning models, to prevent overfitting and improve generalization. It involves monitoring the performance of a model on a validation dataset during training and stopping the training process when the performance on the validation set starts to degrade, even if the performance on the training set continues to improve.\n",
    "\n",
    "Concept of Early Stopping:\n",
    "\n",
    "Training and Validation Sets: During the training process, the dataset is typically split into three parts: training set, validation set, and test set. The training set is used to update the model's parameters, the validation set is used to monitor performance, and the test set is used to evaluate the final model's generalization.\n",
    "\n",
    "Monitoring Validation Performance: As the model trains, its performance on the validation set is periodically evaluated. This can be done after every epoch (complete pass through the training data) or after a certain number of training iterations.\n",
    "\n",
    "Early Stopping Criterion: Early stopping involves setting a criterion that tracks the performance of the model on the validation set. This criterion could be the validation loss, accuracy, or any other relevant metric. The goal is to identify when the validation performance starts to degrade.\n",
    "\n",
    "Patience: The patience parameter determines how many epochs the validation performance can degrade before early stopping is triggered. If the validation performance doesn't improve or worsens for a specified number of epochs (patience), the training process is stopped.\n",
    "\n",
    "How Early Stopping Prevents Overfitting:\n",
    "\n",
    "Generalization Control: During the early stages of training, both the training and validation performances tend to improve. However, as training continues, the model might start overfitting the training data, resulting in a decrease in validation performance. Early stopping detects this point and stops training before the model becomes overly specialized to the training data.\n",
    "\n",
    "Preventing Overfitting: By stopping the training process when the validation performance plateaus or degrades, early stopping prevents the model from adapting too closely to the idiosyncrasies and noise present in the training data. This encourages the model to develop more generalized representations that can better capture patterns in unseen data.\n",
    "\n",
    "Avoiding Training Time Waste: Early stopping helps save computational resources and time. When the model's performance on the validation set shows that it's not improving, continuing to train the model further would likely result in little to no improvement in its generalization performance. Stopping early prevents unnecessary iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50388e3",
   "metadata": {},
   "source": [
    "<b>Q7 Explain the concept of Batch Normalization and its role as a form of regularization. How does Batch\n",
    "Normalization help in preventing overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a16137f",
   "metadata": {},
   "source": [
    "In deep neural networks, the distribution of inputs to each layer (also known as internal covariate shift) can change during training due to changing parameter values. This can slow down training and make it difficult to find good hyperparameters. Batch Normalization addresses this issue by normalizing the inputs of each layer to have a standardized mean and variance across mini-batches during training.\n",
    "\n",
    "Here's how Batch Normalization works:\n",
    "\n",
    "Normalization: For each mini-batch of training data, Batch Normalization standardizes the inputs by subtracting the mini-batch mean and dividing by the mini-batch standard deviation.\n",
    "\n",
    "Scaling and Shifting: After normalization, the standardized inputs are scaled and shifted by learnable parameters (gamma and beta). This allows the network to learn optimal scales and shifts for each normalized feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf333c7",
   "metadata": {},
   "source": [
    "Role of Batch Normalization as Regularization:\n",
    "\n",
    "While Batch Normalization was primarily introduced to accelerate training and mitigate the internal covariate shift problem, it has an inherent regularizing effect that can help prevent overfitting:\n",
    "\n",
    "Noise Injection: The normalization process introduces noise to the inputs of each layer due to the use of mini-batch statistics. This added noise can act as a form of regularization by reducing the reliance of the network on specific noisy patterns in the training data.\n",
    "\n",
    "Smoothing Effects: The normalization process helps in making the optimization process smoother by reducing the impact of outliers and extreme values. This can help the model generalize better to unseen data.\n",
    "\n",
    "Gradient Stabilization: Batch Normalization helps stabilize the gradients during backpropagation, making training more stable and less sensitive to the choice of learning rate.\n",
    "\n",
    "Reduced Internal Covariate Shift: By normalizing the inputs, Batch Normalization reduces the internal covariate shift, which makes it easier for the network to learn meaningful representations. This can prevent the network from memorizing noise and spurious correlations in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c862e1f",
   "metadata": {},
   "source": [
    "## Part 3: Applying Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befb7840",
   "metadata": {},
   "source": [
    "<b>Q8 Implement Dropout regularization in a deep learning model using a framework of your choice. Evaluate\n",
    "its impact on model performance and compare it with a model without Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d564bc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential ,regularizers\n",
    "from tensorflow.keras.layers import Dense , Flatten ,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "161fbb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train),(X_test,y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01e98b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train/255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a491fe68",
   "metadata": {},
   "source": [
    "## without Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de2dfb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28,28)))\n",
    "model.add(Dense(128,activation= 'relu'))\n",
    "model.add(Dense(10,activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cb6b2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.2874 - accuracy: 0.9172 - val_loss: 0.1659 - val_accuracy: 0.9498\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 7s 5ms/step - loss: 0.1297 - accuracy: 0.9616 - val_loss: 0.1319 - val_accuracy: 0.9614\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0897 - accuracy: 0.9731 - val_loss: 0.1046 - val_accuracy: 0.9673\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0657 - accuracy: 0.9800 - val_loss: 0.1020 - val_accuracy: 0.9707\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0515 - accuracy: 0.9841 - val_loss: 0.0967 - val_accuracy: 0.9722\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0398 - accuracy: 0.9876 - val_loss: 0.0934 - val_accuracy: 0.9742\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.0317 - accuracy: 0.9906 - val_loss: 0.0949 - val_accuracy: 0.9736\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.0248 - accuracy: 0.9924 - val_loss: 0.0913 - val_accuracy: 0.9750\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0207 - accuracy: 0.9934 - val_loss: 0.0926 - val_accuracy: 0.9759\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 7s 5ms/step - loss: 0.0178 - accuracy: 0.9946 - val_loss: 0.0978 - val_accuracy: 0.9745\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'sparse_categorical_crossentropy',optimizer = 'Adam',metrics = ['accuracy'])\n",
    "history = model.fit(X_train,y_train,epochs=10,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9932e91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0900 - accuracy: 0.9752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08996598422527313, 0.9751999974250793]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e55212c",
   "metadata": {},
   "source": [
    "## With Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "299982df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28,28)))\n",
    "model.add(Dense(128,activation= 'relu',kernel_regularizer = regularizers.L1L2(l1= 1e-5, l2= 1e-4) ))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10,activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fdbcf356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.1311 - accuracy: 0.9768 - val_loss: 0.1396 - val_accuracy: 0.9752\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.1307 - accuracy: 0.9782 - val_loss: 0.1363 - val_accuracy: 0.9779\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.1289 - accuracy: 0.9782 - val_loss: 0.1351 - val_accuracy: 0.9784\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.1296 - accuracy: 0.9784 - val_loss: 0.1428 - val_accuracy: 0.9745\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.1264 - accuracy: 0.9788 - val_loss: 0.1342 - val_accuracy: 0.9781\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.1218 - accuracy: 0.9803 - val_loss: 0.1420 - val_accuracy: 0.9771\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.1272 - accuracy: 0.9788 - val_loss: 0.1364 - val_accuracy: 0.9777\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 8s 5ms/step - loss: 0.1218 - accuracy: 0.9801 - val_loss: 0.1364 - val_accuracy: 0.9787\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 8s 6ms/step - loss: 0.1241 - accuracy: 0.9804 - val_loss: 0.1379 - val_accuracy: 0.9772\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 8s 6ms/step - loss: 0.1203 - accuracy: 0.9815 - val_loss: 0.1367 - val_accuracy: 0.9798\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'sparse_categorical_crossentropy',optimizer = 'Adam',metrics = ['accuracy'])\n",
    "history = model.fit(X_train,y_train,epochs=10,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c4ec3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 1ms/step - loss: 0.1318 - accuracy: 0.9811\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.13182862102985382, 0.9811000227928162]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a393164f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
