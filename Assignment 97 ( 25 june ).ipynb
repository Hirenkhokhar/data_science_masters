{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17eef837",
   "metadata": {},
   "source": [
    "# TOPIC: Understanding Pooling and Padding in CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a504ab6e",
   "metadata": {},
   "source": [
    "<b>Q1 Describe the purpose and benifits of pooling in CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fc1995",
   "metadata": {},
   "source": [
    "Pooling, also known as subsampling or downsampling, is a crucial operation in Convolutional Neural Networks (CNNs) used for image processing and other types of data with spatial dimensions. The primary purpose of pooling in CNNs is to reduce the spatial dimensions (width and height) of feature maps while retaining the most important information. Pooling serves several important purposes and offers several benefits:\n",
    "\n",
    "1. Dimension Reduction: One of the primary purposes of pooling is to reduce the spatial dimensions of the feature maps. This reduces the computational complexity of the network and helps manage memory usage, making the network more efficient.\n",
    "\n",
    "\n",
    "2. Translation Invariance: Pooling helps the network achieve translation invariance, meaning that it can recognize patterns or features in an image regardless of their exact position. This is crucial for detecting features at different scales and orientations.\n",
    "\n",
    "\n",
    "3. Increased Receptive Field: Pooling allows neurons in deeper layers of the network to have a larger receptive field, which means they can consider a wider region of the input image. This helps in capturing more abstract and global features.\n",
    "\n",
    "\n",
    "4. Feature Selection: Pooling acts as a form of feature selection by retaining the most important information while discarding less relevant or redundant information. This helps prevent overfitting and improves generalization.\n",
    "\n",
    "\n",
    "5. Reduced Overfitting: By reducing the spatial dimensions and retaining only the most salient features, pooling helps in reducing the risk of overfitting, where the model learns to memorize training data rather than generalize from it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868d880e",
   "metadata": {},
   "source": [
    "<b>Q2  Explain the diffecence between min pooling and max pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d200cc",
   "metadata": {},
   "source": [
    "Max pooling and min pooling are two common types of pooling operations used in Convolutional Neural Networks (CNNs) to downsample feature maps, but they differ in how they select values from local regions of the input. Here's a detailed explanation of the differences between max pooling and min pooling:\n",
    "\n",
    "<b>Max Pooling:\n",
    "\n",
    "Operation: Max pooling selects the maximum value from each local region (usually a small square window) of the input feature map.\n",
    "\n",
    "Preservation of Information: Max pooling retains the most prominent or important features within the local region. It is good at capturing features that are most active and relevant.\n",
    "\n",
    "Robustness to Noise: Max pooling is relatively robust to noise in the input data because it focuses on the strongest signal in the region while potentially ignoring small fluctuations.\n",
    "\n",
    "Typical Use Cases: Max pooling is commonly used in CNN architectures for tasks like image classification, where identifying the most prominent features is crucial.\n",
    "\n",
    "<b>Min Pooling:\n",
    "\n",
    "Operation: Min pooling, on the other hand, selects the minimum value from each local region of the input feature map.\n",
    "\n",
    "Preservation of Information: Min pooling retains the least prominent or smallest values within the local region. It tends to capture the less active or less relevant features.\n",
    "\n",
    "Sensitivity to Noise: Min pooling can be sensitive to noise in the input data because it selects the smallest values, which may be affected by small fluctuations in the data.\n",
    "\n",
    "Use Cases: Min pooling is less commonly used in practice compared to max pooling. It might be used in scenarios where capturing the least prominent features is relevant, but these cases are less frequent than those where max pooling is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d944e1c",
   "metadata": {},
   "source": [
    "<b>Q3 Discuss the concept of padding in CNN and its significancet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e166228b",
   "metadata": {},
   "source": [
    "Padding in Convolutional Neural Networks (CNNs) is a technique used to control the spatial dimensions of the output feature maps after applying convolutional operations. Padding involves adding extra pixels or values around the input data before convolution. It is essential for preserving important spatial information and managing the dimensions of the feature maps. Here's a detailed explanation of padding and its significance:\n",
    "\n",
    "Padding Types:\n",
    "There are two common types of padding:\n",
    "\n",
    "Valid (No Padding): In this case, no padding is added to the input data before convolution. As a result, the spatial dimensions of the output feature maps are reduced compared to the input. This is often referred to as \"valid\" convolution.\n",
    "\n",
    "Same (Zero Padding): In \"same\" padding, zeros (or other constant values) are added symmetrically around the input data, so the output feature map has the same spatial dimensions as the input. The term \"same\" indicates that the output size is kept the same as the input size.\n",
    "\n",
    "Significance of Padding:\n",
    "\n",
    "Preservation of Spatial Information: Padding is crucial for preserving spatial information, especially at the edges of an image or feature map. Without padding, the spatial dimensions progressively shrink as you move deeper into the network layers, potentially losing valuable information near the borders.\n",
    "\n",
    "Controlling Output Size: Padding allows you to control the size of the output feature maps. In some cases, you may want to maintain the same spatial dimensions as the input to avoid excessive reduction in size, which could lead to information loss.\n",
    "\n",
    "Centering Convolution: Padding ensures that the convolutional filter's center is aligned with the input data, which is essential for capturing features accurately. Without padding, the filter's center might be positioned entirely on the edge of the input, which can lead to a loss of information.\n",
    "\n",
    "Handling Different Filter Sizes: Padding helps in dealing with convolutional filters of various sizes. It ensures that the filter can be placed at different positions across the input, maintaining consistency in the output size.\n",
    "\n",
    "Mitigating Boundary Effects: Without padding, the edges of the feature maps tend to be less informative because they are affected by fewer neighbors during convolution. Padding can mitigate this issue by allowing the convolution to consider a full neighborhood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f444c7",
   "metadata": {},
   "source": [
    "<b>Q4 Compare and contrast zeco-padding and valid-padding in terms oj their effects on the output\n",
    "featuce map size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2368b73d",
   "metadata": {},
   "source": [
    "Zero-padding (also known as \"same\" padding) and valid-padding are two common padding techniques used in Convolutional Neural Networks (CNNs) to control the size of the output feature maps. They have distinct effects on the output feature map size. Let's compare and contrast these padding techniques in terms of their impact on the output feature map size:\n",
    "\n",
    "<b>Zero-Padding (Same Padding):\n",
    "\n",
    "Effect on Output Size:\n",
    "\n",
    "Zero-padding preserves the spatial dimensions of the input feature map, ensuring that the output feature map has the same spatial dimensions as the input.\n",
    "When you apply convolution with zero-padding, the filter can extend beyond the borders of the input, and the central region of the filter aligns with the center of the input. As a result, the output feature map maintains its size.\n",
    "Use Cases:\n",
    "\n",
    "Zero-padding is commonly used when you want to maintain the spatial dimensions of the feature maps across convolutional layers, especially when transitioning between layers or when you want to avoid excessive reduction in feature map size.\n",
    "Advantages:\n",
    "\n",
    "Preserves spatial information at the borders of the input.\n",
    "Ensures consistent feature extraction across the entire input.\n",
    "Disadvantages:\n",
    "\n",
    "Increases the computational cost of convolution due to the larger input size.\n",
    "    \n",
    "    \n",
    "<B>Valid Padding (No Padding):\n",
    "\n",
    "Effect on Output Size:\n",
    "\n",
    "Valid padding does not add any extra pixels or values around the input data before convolution.\n",
    "Convolution with valid padding results in a reduction in the spatial dimensions of the output feature map compared to the input.\n",
    "Use Cases:\n",
    "\n",
    "Valid padding is often used when you intentionally want to reduce the size of the feature maps. It is common in deep CNN architectures to progressively reduce spatial dimensions to capture higher-level features.\n",
    "Advantages:\n",
    "\n",
    "Reduces computational complexity because there are no additional zero-padding computations.\n",
    "Emphasizes the most informative central regions of the input.\n",
    "Disadvantages:\n",
    "\n",
    "May lead to a loss of spatial information, especially at the borders of the input, where the convolutional filter extends beyond the input borders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72009dd2",
   "metadata": {},
   "source": [
    "# TOPIC: Exploring LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335c78f4",
   "metadata": {},
   "source": [
    "<b>Q1 Provide a brief overview of LeNet-5 architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1ac964",
   "metadata": {},
   "source": [
    "LeNet-5 is a convolutional neural network (CNN) architecture developed by Yann LeCun and his colleagues in the 1990s. It played a significant role in the advancement of deep learning and computer vision, particularly in the recognition of handwritten digits and characters. Here's a brief overview of the LeNet-5 architecture:\n",
    "\n",
    "Input Layer:\n",
    "\n",
    "LeNet-5 typically takes grayscale images as input, with a size of 32x32 pixels.\n",
    "Convolutional Layers:\n",
    "\n",
    "The network consists of two convolutional layers followed by max-pooling layers.\n",
    "The first convolutional layer uses a 5x5 kernel and applies 6 filters.\n",
    "The second convolutional layer uses a 5x5 kernel and applies 16 filters.\n",
    "Both convolutional layers use the \"tanh\" activation function.\n",
    "Subsampling (Max-Pooling) Layers:\n",
    "\n",
    "After each convolutional layer, a max-pooling layer is applied.\n",
    "The first max-pooling layer uses a 2x2 window with a stride of 2.\n",
    "The second max-pooling layer uses a 2x2 window with a stride of 2.\n",
    "Fully Connected Layers:\n",
    "\n",
    "Following the convolutional and max-pooling layers, there are three fully connected layers.\n",
    "The first fully connected layer has 120 neurons with a \"tanh\" activation function.\n",
    "The second fully connected layer has 84 neurons with a \"tanh\" activation function.\n",
    "The final output layer has 10 neurons (one for each class in the MNIST dataset) with a softmax activation function for classification.\n",
    "Output Layer:\n",
    "\n",
    "The output layer produces a probability distribution over the 10 possible classes in the MNIST dataset, which represents digits from 0 to 9.\n",
    "Training:\n",
    "\n",
    "LeNet-5 is trained using backpropagation and gradient descent, typically with a cross-entropy loss function.\n",
    "It was originally designed for digit recognition tasks, such as the MNIST dataset.\n",
    "LeNet-5 was a groundbreaking architecture at the time of its development, and it demonstrated the effectiveness of CNNs for image classification tasks. While it has since been surpassed by deeper and more complex CNN architectures, it remains an important historical milestone in the development of deep learning for computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429dd158",
   "metadata": {},
   "source": [
    "<B>Q2 Describe the key components of LeNet-5 and their respective purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968f9a93",
   "metadata": {},
   "source": [
    "\n",
    "LeNet-5 is a classic convolutional neural network (CNN) architecture designed for image recognition tasks, particularly for handwritten digit recognition. It consists of several key components, each with its own specific purpose:\n",
    "\n",
    "Input Layer:\n",
    "\n",
    "Purpose: The input layer is where the network receives the raw image data.\n",
    "Description: LeNet-5 typically takes grayscale images as input, which are represented as 32x32 pixel grids. Each pixel's intensity value serves as an input neuron.\n",
    "Convolutional Layers:\n",
    "\n",
    "Purpose: Convolutional layers are responsible for feature extraction by applying convolution operations to the input.\n",
    "Description: LeNet-5 has two convolutional layers. The first layer uses a 5x5 kernel and applies 6 filters, while the second layer uses a 5x5 kernel and applies 16 filters. These layers help detect various features and patterns in the input images, such as edges and simple textures.\n",
    "Subsampling (Max-Pooling) Layers:\n",
    "\n",
    "Purpose: Subsampling layers reduce the spatial dimensions of the feature maps while preserving important information.\n",
    "Description: After each convolutional layer, LeNet-5 applies max-pooling layers. These layers use a 2x2 window and a stride of 2 to downsample the feature maps, reducing their size and providing a degree of translation invariance.\n",
    "Fully Connected Layers:\n",
    "\n",
    "Purpose: Fully connected layers combine the extracted features to make final predictions.\n",
    "Description: LeNet-5 includes three fully connected layers. The first fully connected layer has 120 neurons, the second has 84 neurons, and the final output layer has 10 neurons. These layers gradually reduce the dimensionality of the features and eventually produce class probabilities using activation functions like \"tanh\" and softmax.\n",
    "Activation Functions:\n",
    "\n",
    "Purpose: Activation functions introduce non-linearity into the network, allowing it to model complex relationships in the data.\n",
    "Description: LeNet-5 primarily uses the \"tanh\" activation function in its convolutional and fully connected layers. The output layer uses softmax to convert the final layer's raw scores into class probabilities.\n",
    "Output Layer:\n",
    "\n",
    "Purpose: The output layer provides the final classification results.\n",
    "Description: In LeNet-5, the output layer has 10 neurons, each representing a digit class (0 to 9). The softmax activation function is applied to produce a probability distribution over these classes, indicating the network's confidence in its predictions.\n",
    "Training:\n",
    "\n",
    "Purpose: The network is trained to learn optimal weights and biases for accurate classification.\n",
    "Description: LeNet-5 is trained using backpropagation and gradient descent with a suitable loss function, typically cross-entropy. During training, the network adjusts its parameters to minimize the error between its predictions and the ground truth labels in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1029f11e",
   "metadata": {},
   "source": [
    "<b>Q3 Discuss the advantages and limitations of LeNet-5 in the context of image classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d91cf44",
   "metadata": {},
   "source": [
    "<b>Advantages of LeNet-5:\n",
    "\n",
    "Simplicity: LeNet-5 is a relatively simple CNN architecture compared to more modern architectures like ResNet or Inception. Its simplicity makes it easier to understand and implement, which can be advantageous for educational purposes and when dealing with limited computational resources.\n",
    "\n",
    "Effective for Small Datasets: LeNet-5 performs well on small to medium-sized datasets. It was originally designed for handwritten digit recognition, such as the MNIST dataset, and has been successfully used for similar tasks.\n",
    "\n",
    "Convolution and Pooling Layers: LeNet-5 introduced the concept of convolutional and pooling layers, which are fundamental components of modern CNNs. These layers allow the network to automatically learn and extract hierarchical features from input images, making it effective for capturing spatial hierarchies in data.\n",
    "\n",
    "Weight Sharing: LeNet-5 uses weight sharing, which reduces the number of learnable parameters in the network. This reduces the risk of overfitting, especially when the dataset is small, and helps the network generalize better.\n",
    "\n",
    "Translation Invariance: LeNet-5 exhibits translation invariance, meaning it can recognize patterns and features in different positions within an image. This property is crucial for tasks where the location of features is not fixed.\n",
    "\n",
    "<B>Limitations of LeNet-5:\n",
    "\n",
    "Limited Depth: LeNet-5 has a relatively shallow architecture compared to modern CNNs. Deeper networks have shown to be more effective at learning complex and abstract features, which is important for tasks involving high-resolution images or intricate patterns.\n",
    "\n",
    "Lack of Non-linearity: LeNet-5 uses the sigmoid activation function, which has limitations compared to more modern activation functions like ReLU (Rectified Linear Unit). ReLU is known to accelerate training and help networks converge faster.\n",
    "\n",
    "Limited Capacity: Due to its architecture, LeNet-5 may not perform well on extremely large and diverse datasets. More complex architectures are better suited for handling the diversity of features and patterns in such datasets.\n",
    "\n",
    "Not Suitable for Modern Tasks: LeNet-5 was designed in an era when image classification tasks were less complex. Today, tasks like object detection, semantic segmentation, and image generation require more advanced architectures that can handle diverse and challenging scenarios.\n",
    "\n",
    "Lack of Skip Connections: LeNet-5 does not incorporate skip connections or residual connections, which have proven to be highly effective in improving the training and performance of deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe3b064",
   "metadata": {},
   "source": [
    "<b>Q4 Implement LeNet-5 using a deep leacning framework of your choice (e.g., TensocFlow, PyTorch) and train it on a publicly available dataset (e.g., MNIST). Evaluate its performance and provide\n",
    "insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f1950ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 6)         456       \n",
      "                                                                 \n",
      " average_pooling2d (Average  (None, 14, 14, 6)         0         \n",
      " Pooling2D)                                                      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 10, 10, 16)        2416      \n",
      "                                                                 \n",
      " average_pooling2d_1 (Avera  (None, 5, 5, 16)          0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 400)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 120)               48120     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 84)                10164     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 62006 (242.21 KB)\n",
      "Trainable params: 62006 (242.21 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "391/391 [==============================] - 26s 59ms/step - loss: 1.8502 - accuracy: 0.3401 - val_loss: 1.7224 - val_accuracy: 0.3913\n",
      "Epoch 2/2\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.6758 - accuracy: 0.4079 - val_loss: 1.6086 - val_accuracy: 0.4213\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.6086 - accuracy: 0.4213\n",
      "Test Loss: 1.608552098274231\n",
      "Test accuracy: 0.4212999939918518\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Conv2D, MaxPooling2D,AveragePooling2D\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values between 0 and 1\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "\n",
    "# Building the Model Architecture\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(6, kernel_size = (5,5), padding = 'valid', activation='tanh', input_shape = (32,32,3)))\n",
    "model.add(AveragePooling2D(pool_size= (2,2), strides = 2, padding = 'valid'))\n",
    "\n",
    "model.add(Conv2D(16, kernel_size = (5,5), padding = 'valid', activation='tanh'))\n",
    "model.add(AveragePooling2D(pool_size= (2,2), strides = 2, padding = 'valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(120, activation='tanh'))\n",
    "model.add(Dense(84, activation='tanh'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(loss=keras.metrics.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=2, verbose=1, validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test)\n",
    "\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0ed29d",
   "metadata": {},
   "source": [
    "# TOPIC: Analyzing AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e69815",
   "metadata": {},
   "source": [
    "<B>Q1 Present an overview of the AlexNet architectuce\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f8a37e",
   "metadata": {},
   "source": [
    "AlexNet is a pioneering deep convolutional neural network (CNN) architecture that gained significant attention and marked a breakthrough in the field of computer vision when it won the ImageNet Large Scale Visual Recognition Challenge in 2012. Here's an overview of the AlexNet architecture:\n",
    "\n",
    "Input Layer:\n",
    "\n",
    "AlexNet takes an input image of size 227x227 pixels with three color channels (RGB).\n",
    "Convolutional Layers:\n",
    "\n",
    "The network starts with five convolutional layers, followed by max-pooling layers.\n",
    "The convolutional layers use a relatively large receptive field (filter size of 11x11 pixels in the first layer) compared to modern architectures to capture complex patterns.\n",
    "The first convolutional layer has 96 filters, while the subsequent layers have 256, 384, and 384 filters, respectively.\n",
    "The rectified linear unit (ReLU) activation function is used after each convolutional layer to introduce non-linearity.\n",
    "Max-Pooling Layers:\n",
    "\n",
    "After each of the first two convolutional layers, there is a max-pooling layer with a 3x3 pixel window and a stride of 2.\n",
    "Max-pooling reduces the spatial dimensions while retaining important features.\n",
    "Local Response Normalization:\n",
    "\n",
    "After the first and second convolutional layers, there is a local response normalization (LRN) layer.\n",
    "LRN helps enhance the contrast between different features in the same location and is thought to promote generalization.\n",
    "Fully Connected Layers:\n",
    "\n",
    "Following the convolutional and max-pooling layers, there are three fully connected layers.\n",
    "The first two fully connected layers have 4096 neurons each, while the final fully connected layer has 1000 neurons, corresponding to the 1000 classes in the ImageNet dataset.\n",
    "Dropout is applied to the two larger fully connected layers during training to prevent overfitting.\n",
    "Output Layer:\n",
    "\n",
    "The output layer is a softmax layer that assigns probabilities to the 1000 ImageNet classes.\n",
    "It produces the final classification results.\n",
    "Training Details:\n",
    "\n",
    "AlexNet was trained using stochastic gradient descent (SGD) with a relatively high learning rate.\n",
    "Data augmentation techniques, such as random cropping and flipping, were used during training to improve generalization.\n",
    "The network was trained on a large-scale dataset (ImageNet) containing millions of images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d876f24e",
   "metadata": {},
   "source": [
    "<b>Q2 Explain the architectural innovations introduced in AlexNet that contributed to its breakthrough\n",
    "pecformance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882fde47",
   "metadata": {},
   "source": [
    "\n",
    "AlexNet achieved a breakthrough in performance in the ImageNet Large Scale Visual Recognition Challenge in 2012, primarily due to several architectural innovations that significantly improved the capabilities of deep convolutional neural networks (CNNs). These innovations contributed to its remarkable success:\n",
    "\n",
    "Deep Architecture:\n",
    "\n",
    "AlexNet introduced a much deeper architecture compared to previous CNNs. It consisted of eight layers: five convolutional layers followed by three fully connected layers. Prior to AlexNet, shallower networks were more common. The depth allowed the network to learn more complex and hierarchical features, which proved crucial for recognizing intricate patterns in images.\n",
    "Large Receptive Fields:\n",
    "\n",
    "The first convolutional layer in AlexNet used a large receptive field with an 11x11 filter size. This large filter size helped capture high-level features in the input image, such as edges, corners, and textures. Subsequent layers used smaller filter sizes to capture finer details.\n",
    "Multiple Convolutional Layers:\n",
    "\n",
    "AlexNet employed multiple convolutional layers, each followed by ReLU activation functions. The use of multiple layers allowed the network to learn increasingly abstract and sophisticated features. This deep representation enabled the network to recognize complex patterns and objects.\n",
    "Max-Pooling Layers:\n",
    "\n",
    "After the first two convolutional layers, AlexNet included max-pooling layers with a 3x3 window and a stride of 2. Max-pooling helped reduce the spatial dimensions of the feature maps, making the network more computationally efficient and robust to variations in object positions within the images.\n",
    "Local Response Normalization (LRN):\n",
    "\n",
    "AlexNet incorporated LRN layers after the first and second convolutional layers. LRN enhances the contrast between different features in the same location, which is thought to be beneficial for generalization. However, LRN has been largely replaced by batch normalization in modern architectures.\n",
    "Dropout Regularization:\n",
    "\n",
    "To prevent overfitting, AlexNet used dropout in the fully connected layers. Dropout randomly deactivates a fraction of neurons during each forward and backward pass, forcing the network to learn more robust features and reducing the risk of overfitting.\n",
    "Data Augmentation:\n",
    "\n",
    "During training, AlexNet applied data augmentation techniques, including random cropping and horizontal flipping of training images. This helped the network generalize better by exposing it to a wider variety of image variations.\n",
    "Parallelism:\n",
    "\n",
    "AlexNet was trained on two GPUs simultaneously, which allowed for parallel processing of the data and gradients. This approach significantly reduced training time and made it feasible to train deep networks efficiently.\n",
    "Large-Scale Dataset:\n",
    "\n",
    "AlexNet was trained on the large-scale ImageNet dataset, which contained millions of labeled images across thousands of categories. The availability of this massive dataset was instrumental in training a deep network like AlexNet effectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a859f3e",
   "metadata": {},
   "source": [
    "<b>Q3  Discuss the role of convolutional layers, pooling layers, and fully connected layers in AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5f2710",
   "metadata": {},
   "source": [
    "Convolutional Layers:\n",
    "\n",
    "Feature Extraction: The convolutional layers in AlexNet are responsible for feature extraction. They apply convolutional filters (kernels) to the input image to detect various patterns and features, such as edges, textures, and simple shapes. These filters slide over the entire input image to produce feature maps.\n",
    "\n",
    "Pooling Layers:\n",
    "\n",
    "Dimension Reduction: After some of the convolutional layers, AlexNet includes max-pooling layers. These layers reduce the spatial dimensions (width and height) of the feature maps while retaining the most important information. Max-pooling is performed by taking the maximum value within a local region (e.g., a 2x2 or 3x3 window) and moving it across the feature map.\n",
    "\n",
    "Fully Connected Layers:\n",
    "\n",
    "High-Level Representation: The fully connected layers at the end of the network take the high-level features extracted by the convolutional and pooling layers and transform them into a form suitable for making class predictions. These layers are responsible for learning complex relationships between features.\n",
    "\n",
    "Classification: The final fully connected layer in AlexNet consists of 1000 neurons, corresponding to the 1000 classes in the ImageNet dataset. It applies a softmax activation function to produce class probabilities, effectively making predictions about which object or category is present in the input image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3906b93c",
   "metadata": {},
   "source": [
    "<b>Q4 Implement AlexNet using a deep leacning  of your framewarok choice and evaluate its performance\n",
    "on a dataset of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf73c631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1758744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1360, 17)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = np.load(\"E:/Data science/Dataset/oxflower17.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ac05d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_9 (Conv2D)           (None, 54, 54, 96)        34944     \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 54, 54, 96)        0         \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 26, 26, 96)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 26, 26, 96)        384       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 26, 26, 256)       614656    \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 26, 26, 256)       0         \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPoolin  (None, 12, 12, 256)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 12, 12, 256)       1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 10, 10, 384)       885120    \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 10, 10, 384)       0         \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 10, 10, 384)       1536      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 8, 8, 384)         1327488   \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 8, 8, 384)         0         \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 8, 8, 384)         1536      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 6, 6, 256)         884992    \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 2, 2, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_12 (Ba  (None, 2, 2, 256)         1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 4096)              4198400   \n",
      "                                                                 \n",
      " activation_15 (Activation)  (None, 4096)              0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " batch_normalization_13 (Ba  (None, 4096)              16384     \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " activation_16 (Activation)  (None, 4096)              0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " batch_normalization_14 (Ba  (None, 4096)              16384     \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 17)                69649     \n",
      "                                                                 \n",
      " activation_17 (Activation)  (None, 17)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24834833 (94.74 MB)\n",
      "Trainable params: 24815697 (94.66 MB)\n",
      "Non-trainable params: 19136 (74.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# 1st Convolutional Layer\n",
    "model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11), strides=(4,4), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Pooling\n",
    "model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
    "# Batch Normalisation before passing it to the next layer\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 2nd Convolutional Layer\n",
    "model.add(Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Pooling\n",
    "model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "\n",
    "# 3rd Convolutional Layer\n",
    "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 4th Convolutional Layer\n",
    "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "# 5th Convolutional Layer\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "# Pooling\n",
    "model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "# Passing it to a dense layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# 1st Dense Layer\n",
    "model.add(Dense(4096, input_shape=(224*224*3,)))\n",
    "model.add(Activation('relu'))\n",
    "# Add Dropout to prevent overfitting\n",
    "model.add(Dropout(0.4))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 2nd Dense Layer\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation('relu'))\n",
    "# Add Dropout\n",
    "model.add(Dropout(0.4))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(17))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "423a3f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a1db429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1088 samples, validate on 272 samples\n",
      "Epoch 1/10\n",
      "1088/1088 [==============================] - 33s 30ms/sample - loss: 1.3998 - acc: 0.6048 - val_loss: 3.6457 - val_acc: 0.2904\n",
      "Epoch 2/10\n",
      "1088/1088 [==============================] - 42s 38ms/sample - loss: 1.3443 - acc: 0.6131 - val_loss: 6.6661 - val_acc: 0.1985\n",
      "Epoch 3/10\n",
      "1088/1088 [==============================] - 43s 40ms/sample - loss: 1.2533 - acc: 0.6250 - val_loss: 2.4072 - val_acc: 0.4375\n",
      "Epoch 4/10\n",
      "1088/1088 [==============================] - 38s 35ms/sample - loss: 0.9790 - acc: 0.6912 - val_loss: 2.6102 - val_acc: 0.4154\n",
      "Epoch 5/10\n",
      "1088/1088 [==============================] - 35s 32ms/sample - loss: 0.9524 - acc: 0.7215 - val_loss: 2.8711 - val_acc: 0.4228\n",
      "Epoch 6/10\n",
      "1088/1088 [==============================] - 42s 39ms/sample - loss: 0.8479 - acc: 0.7362 - val_loss: 3.1144 - val_acc: 0.4081\n",
      "Epoch 7/10\n",
      "1088/1088 [==============================] - 35s 32ms/sample - loss: 0.7689 - acc: 0.7564 - val_loss: 3.6598 - val_acc: 0.3787\n",
      "Epoch 8/10\n",
      "1088/1088 [==============================] - 36s 33ms/sample - loss: 0.6697 - acc: 0.8015 - val_loss: 2.9225 - val_acc: 0.4485\n",
      "Epoch 9/10\n",
      "1088/1088 [==============================] - 36s 33ms/sample - loss: 0.4877 - acc: 0.8557 - val_loss: 2.3830 - val_acc: 0.4853\n",
      "Epoch 10/10\n",
      "1088/1088 [==============================] - 36s 33ms/sample - loss: 0.4229 - acc: 0.8585 - val_loss: 3.3699 - val_acc: 0.3897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x213decadf90>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "model.fit(data['X'],data['Y'], batch_size=64, epochs=10, verbose=1,validation_split=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae07fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8856706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
